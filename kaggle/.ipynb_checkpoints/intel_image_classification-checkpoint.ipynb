{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1563b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de2fc0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7ea7d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(train):\n",
    "    transform = []\n",
    "    transform.append(transforms.PILToTensor())\n",
    "    transform.append(transforms.ConvertImageDtype(torch.float))\n",
    "    transform.append(transforms.Resize(150,150))\n",
    "    if train:\n",
    "        transform.append(transforms.RandomRotation(20, fill=0))\n",
    "        transform.append(transforms.RandomHorizontalFlip(0.5))\n",
    "    \n",
    "    return transforms.Compose(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8dd69d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, img_path, transformer):\n",
    "        \n",
    "        self.img_path = img_path\n",
    "        self.transformer = transformer\n",
    "        self.file_names = os.listdir(self.img_path)\n",
    "        self.all_image_paths = []\n",
    "        \n",
    "        \n",
    "#         self.classes = []\n",
    "\n",
    "        for i in range(len(self.file_names)):\n",
    "            self.all_image_paths.extend(glob.glob(self.img_path+self.file_names[i]+'/*jpg'))\n",
    "#             for j in range(len(glob.glob(self.img_path+self.file_names[i]+'/*jpg'))):\n",
    "#                 self.classes.append(i)\n",
    "        self.idx_to_class = {i:j for i, j in enumerate(self.file_names)}\n",
    "        self.class_to_idx = {value:key for key,value in idx_to_class.items()}\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_path = self.all_image_paths\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        label = {i:j for i, j in enumerate(self.file_names)}\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "            \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41ccd45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_maker(cfg):\n",
    "    in_channel = 32\n",
    "    layer = []\n",
    "    \n",
    "    for v in cfg:\n",
    "        t, out_channel, n, stride = v\n",
    "        \n",
    "        for i in range(n):\n",
    "            \n",
    "            if i != 0:\n",
    "                stride = 1\n",
    "                \n",
    "            conv_pointwise1 = nn.Conv2d(in_channels=in_channel, \n",
    "                                        out_channels=in_channel*t,\n",
    "                                        kernel_size=1,\n",
    "                                        stride=1, \n",
    "                                        padding=0)\n",
    "            \n",
    "            conv_depthwise = nn.Conv2d(in_channels=in_channel*t,\n",
    "                                      out_channels=in_channel*t,\n",
    "                                      kernel_size=3,\n",
    "                                      groups=in_channel*t,\n",
    "                                      stride=stride,\n",
    "                                      padding=1)\n",
    "            \n",
    "            conv_pointwise2 = nn.Conv2d(in_channels=in_channel*t, \n",
    "                                        out_channels=out_channel,\n",
    "                                        kernel_size=1,\n",
    "                                        stride=1, \n",
    "                                        padding=0)\n",
    "            \n",
    "            layer.append([conv_pointwise1, nn.BatchNorm2d(in_channel*t), nn.ReLU6(),\n",
    "                         conv_depthwise, nn.BatchNorm2d(in_channel*t), nn.ReLU6(),\n",
    "                         conv_pointwise2, nn.BatchNorm2d(out_channel)])\n",
    "            \n",
    "            in_channel = out_channel\n",
    "            \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "006af1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mobilenet(nn.Module):\n",
    "    def __init__(self, features, stride_list, num_classes = 6):\n",
    "        super(Mobilenet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3,32,kernel_size=3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(32))\n",
    "        \n",
    "        self.bottle_neck = []\n",
    "        for layer in features:\n",
    "            self.bottle_neck.append(nn.Sequential(*layer).cpu())\n",
    "            \n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(320, 1280,kernel_size=1, stride=1, padding=0),\n",
    "                                   nn.BatchNorm2d(1280),\n",
    "                                   nn.AvgPool2d(5))\n",
    "        \n",
    "        self.fc = nn.Linear(1280, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        \n",
    "        for i in range(len(self.bottle_neck)):\n",
    "            new = self.bottle_neck[i](out)\n",
    "            if stride_list[i] == 1:\n",
    "                if out.shape[1] != new.shape[1]:\n",
    "                    out = Downsample(out.shape[1], new.shape[1]).cpu()(out)\n",
    "                new += out\n",
    "            out = new\n",
    "            \n",
    "        out = self.layer2(out)\n",
    "        \n",
    "        out = out.view(out.sizw(0), -1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(nn.Conv2d(in_channel, out_channel, kernel_size=1),\n",
    "                                  nn.BatchNorm2d(out_channel))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer(x)\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5cf0b4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1)),\n",
       "  BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  ReLU6(),\n",
       "  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32),\n",
       "  BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  ReLU6(),\n",
       "  Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1)),\n",
       "  BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {'MobileNetV2':[(1,16,1,1),\n",
    "                     (6,24,2,2),\n",
    "                     (6,32,3,2),\n",
    "                     (6,64,4,2),\n",
    "                     (6,96,3,1),\n",
    "                     (6,160,3,2),\n",
    "                      (6,320,1,1)]}\n",
    "bottleneck = layer_maker(cfg['MobileNetV2'])\n",
    "bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d334d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stride_list = [1,\n",
    "              2,1,\n",
    "              2,1,1,\n",
    "              2,1,1,1,\n",
    "              1,1,1,\n",
    "              2,1,1,\n",
    "              1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "efb6b3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mobilenet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): AvgPool2d(kernel_size=5, stride=5, padding=0)\n",
       "  )\n",
       "  (fc): Linear(in_features=1280, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Mobilenet(bottleneck, stride_list, num_classes=6).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cee6ecb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [1280, 320, 1, 1], expected input[2, 16, 75, 75] to have 320 channels, but got 16 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fl/lytw3yqx5913yh2tcxgw3d740000gn/T/ipykernel_7070/3223389196.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout_t_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_t_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_t_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fl/lytw3yqx5913yh2tcxgw3d740000gn/T/ipykernel_7070/986204769.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [1280, 320, 1, 1], expected input[2, 16, 75, 75] to have 320 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "t_img = torch.Tensor(2,3,150,150).to(device)\n",
    "out_t_img = model(t_img)\n",
    "print(out_t_img.shape)\n",
    "print(out_t_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4240867",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2fd47227",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Image.open('./dataset/archive/seg_train/seg_train/forest/10007.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2280cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = transforms.Compose([transforms.PILToTensor(),\n",
    "                       transforms.ConvertImageDtype(torch.float)])\n",
    "img = t(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0dc4b61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 150, 150])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "423a78e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5, 5])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con1 = nn.Conv2d(3,32,kernel_size=3, stride=2, padding=1)\n",
    "con2 = nn.Conv2d(32,32,kernel_size=3, stride=2, padding=1)\n",
    "img1 = con1(img)\n",
    "img1 = con2(img1)\n",
    "img1 = con2(img1)\n",
    "img1 = con2(img1)\n",
    "img1 = con2(img1)\n",
    "img1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943d797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
